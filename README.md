## Solving Obstacle Avoidance in custom enviroment with Reinforcement Learning and Deep Neural Network 

Degree's thesis about Reinforcement Learning (RL), Deep RL and Optimization and dynamic environment.

- **Author**: Nang Yune Thitsar

- **Tuto**r: Professor Hu, Guoqiang

- **University**: Nanyang Technological University

- **Master**: Bachelor Degree in School of Electrical and Electronic Engineering

  

  ## Abstract

Machine Learning (ML) has become a very popular field in Data Science due to the increase in computation power in recent years. It is usually said that ML techniques divide in two types, Supervised Learning and Unsupervised Learning. However, this is not a complete classification. Apart from these two types, we must distinguish another one which is very different from those two: Reinforcement Learning (RL). RL consists of techniques that have been used for decades in the field of Artificial Intelligence for many applications in fields such as robotics and industrial automation [1], health and medicine, Media and Advertising , Finance, text, speech and dialog systems , and so forth.

In this thesis  we present a novel approach for solving product delivery problems by means of Reinforcement Learning and Deep Neural Networks (DNN), a field also referred to as Deep Reinforcement Learning (DRL)[4]. In here we have developed an custom OpenAI gym environment for our dynamic obstacle allocation. We also have developed our custom deep learning model using transfer learning. Our GitHub repository can be found in [here](https://github.com/yunethitsar/Obstacle-avoiding-robot-RL).



## Prerequisites

- Python (tested for python 3.8)
- The gym-pdsystem python package is needed due to some of the python libraries that are found there. Just clone the repository from [here](https://github.com/dsalgador/gym-pdsystem/tree/master/gym_pdsystem)
- Tensorflow library
- matplotlib
- opencv-python

## License

This work is under a MIT  [license](https://github.com/yunethitsar/Obstacle-avoiding-robot-RL/blob/main/LICENSE)



## References

- [1] Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998.

- [2] Michael J. Frank, Lauren C. Seeberger, and Randall C. O’Reilly. By carrot or by stick: Cog- nitive reinforcement learning in parkinsonism. Science, 306(5703):1940–1943, 2004.
- [3] Zhao Yufan, Kosorok Michael R., and Zeng Donglin. Reinforcement learning design for cancer clinical trials. Statistics in Medicine, 28(26):3294–3315.
- [4] Francesco Bertoluzzo and Marco Corazza. Reinforcement learning for automated financial trading: Basics and applications. In Simone Bassis, Anna Esposito, and Francesco Carlo Morabito, editors, Recent Advances of Neural Network Models and Applications, pages 197– 213, Cham, 2014. Springer International Publishing.
