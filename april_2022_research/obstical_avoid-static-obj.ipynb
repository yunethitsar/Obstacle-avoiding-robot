{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install gym\n",
    "#!pip install matplotlib\n",
    "#!pip install opencv-python\n",
    "#!apt-get install ffmpeg libsm6 libxext6  -y\n",
    "#!apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(project=\"rl_object_avoid\", entity=\"tharhtetsan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "from gym import Env, spaces\n",
    "import time\n",
    "font = cv2.FONT_HERSHEY_COMPLEX_SMALL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point(object):\n",
    "    def __init__(self, name, x_max, x_min, y_max, y_min):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "        self.name = name\n",
    "    \n",
    "    def set_position(self, x, y):\n",
    "        self.x = self.clamp(x, self.x_min, self.x_max - self.icon_w)\n",
    "        self.y = self.clamp(y, self.y_min, self.y_max - self.icon_h)\n",
    "    \n",
    "    def get_position(self):\n",
    "        return (self.x, self.y)\n",
    "    \n",
    "    def move(self, del_x, del_y):\n",
    "        self.x += del_x\n",
    "        self.y += del_y\n",
    "        \n",
    "        self.x = self.clamp(self.x, self.x_min, self.x_max - self.icon_w)\n",
    "        self.y = self.clamp(self.y, self.y_min, self.y_max - self.icon_h)\n",
    "\n",
    "    def clamp(self, n, minn, maxn):\n",
    "        return max(min(maxn, n), minn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot(Point):\n",
    "    def __init__(self, name, x_max, x_min, y_max, y_min):\n",
    "        super(Robot, self).__init__(name, x_max, x_min, y_max, y_min)\n",
    "        self.icon = cv2.imread(\"robot.PNG\") / 255.0\n",
    "        self.icon_w = 20\n",
    "        self.icon_h = 20\n",
    "        self.icon = cv2.resize(self.icon, (self.icon_h, self.icon_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Obstical(Point):\n",
    "    def __init__(self, name, x_max, x_min, y_max, y_min):\n",
    "        super(Obstical, self).__init__(name, x_max, x_min, y_max, y_min)\n",
    "        self.icon = cv2.imread(\"building.png\") / 255.0\n",
    "        self.icon_w = 20\n",
    "        self.icon_h = 20\n",
    "        self.icon = cv2.resize(self.icon, (self.icon_h, self.icon_w))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flag(Point):\n",
    "    def __init__(self, name, x_max, x_min, y_max, y_min):\n",
    "        super(Flag, self).__init__(name, x_max, x_min, y_max, y_min)\n",
    "        self.icon = cv2.imread(\"flag.PNG\") / 255.0\n",
    "        self.icon_w = 32\n",
    "        self.icon_h = 32\n",
    "        self.icon = cv2.resize(self.icon, (self.icon_h, self.icon_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot_Env(Env):\n",
    "    def __init__(self):\n",
    "        super(Robot_Env, self).__init__()\n",
    "        \n",
    "        # Define a 2-D observation space\n",
    "        self.observation_shape = (400, 400, 3)\n",
    "        self.observation_space = spaces.Box(low = np.zeros(self.observation_shape), \n",
    "                                            high = np.ones(self.observation_shape),\n",
    "                                            dtype = np.float16)\n",
    "        \n",
    "        # Define an action space ranging from 0 to 4\n",
    "        self.action_space = spaces.Discrete(6,)\n",
    "        \n",
    "         # Create a canvas to render the environment images upon \n",
    "        self.canvas = np.ones(self.observation_shape) * 1\n",
    "        \n",
    "        self.elements = []\n",
    "        self.max_fuel = 150\n",
    "        \n",
    "        \n",
    "        self.y_min = int (self.observation_shape[0] * 0.1)\n",
    "        self.x_min = 0\n",
    "        self.y_max = int (self.observation_shape[0] * 0.9)\n",
    "        self.x_max = self.observation_shape[1]\n",
    "       \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        assert self.action_space.contains(action), \"Invalid Action\"\n",
    "        # Decrease the fuel counter \n",
    "        self.fuel_left -= 1 \n",
    "        # Reward for executing a step.\n",
    "       \n",
    "        \n",
    "        \n",
    "        if action == 0:\n",
    "            self.robot.move(0,5)\n",
    "        elif action == 1:\n",
    "            self.robot.move(0,-5)\n",
    "        elif action == 2:\n",
    "            self.robot.move(5,0)\n",
    "        elif action == 3:\n",
    "            self.robot.move(-5,0)\n",
    "        elif action == 4:\n",
    "            self.robot.move(0,0)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        reward = 0 \n",
    "        \n",
    "        for elem in self.elements:\n",
    "            ## check robot hit building\n",
    "            if isinstance(elem, Obstical):\n",
    "                if self.has_collided(self.robot, elem):\n",
    "                    # Conclude the episode and remove the chopper from the Env.\n",
    "                    done = True\n",
    "                    reward = -10\n",
    "                    #self.elements.remove(self.robot)\n",
    "\n",
    "            elif isinstance(elem, Flag):\n",
    "                if self.has_collided(self.robot, elem):\n",
    "                    done = True\n",
    "                    reward = 100\n",
    "                    self.elements.remove(self.robot)\n",
    "\n",
    "                else:\n",
    "                    robot_x, robot_y = self.robot.get_position()\n",
    "                    goal_x,goal_y = elem.get_position()\n",
    "                    dist =  math.sqrt((goal_x -robot_x)**2+(goal_y-robot_y)**2)\n",
    "                    reward = (1/dist) * 1000\n",
    "                        \n",
    "                        \n",
    "\n",
    "                \n",
    "    \n",
    "         # Increment the episodic return\n",
    "        self.ep_return += reward\n",
    "        \n",
    "        # Draw elements on the canvas\n",
    "        self.draw_elements_on_canvas()\n",
    "        \n",
    "        # If out of fuel, end the episode.\n",
    "        if self.fuel_left == 0:\n",
    "            done = True\n",
    "\n",
    "        return self.canvas, reward, done, []\n",
    "            \n",
    "    \n",
    "    \n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the fuel consumed\n",
    "        self.fuel_left = self.max_fuel\n",
    "\n",
    "        # Reset the reward\n",
    "        self.ep_return  = 0\n",
    "        \n",
    "        \n",
    "        # Number of birds\n",
    "        self.obstical_count = 0\n",
    "        \n",
    "        \n",
    "        x = random.randrange(int(self.observation_shape[0] * 0.05), int(self.observation_shape[0] * 0.10))\n",
    "        y = random.randrange(int(self.observation_shape[1] * 0.15), int(self.observation_shape[1] * 0.20))\n",
    "        \n",
    "        self.robot = Robot(\"Robot\", self.x_max, self.x_min, self.y_max, self.y_min)\n",
    "        self.robot.set_position(x,y)\n",
    "        \n",
    "        \n",
    "         # Intialise the elements \n",
    "        self.elements = [self.robot]\n",
    "        \n",
    "        \n",
    "        # Reset the Canvas \n",
    "        self.canvas = np.ones(self.observation_shape) * 1\n",
    "\n",
    "        # Draw elements on the canvas\n",
    "        self.draw_elements_on_canvas()\n",
    "\n",
    "\n",
    "        # return the observation\n",
    "        return self.canvas \n",
    "    \n",
    "    \n",
    "    \n",
    "    def set_obsticals(self,points):\n",
    "        for obstical_x, obstical_y in points:\n",
    "            obstical_obj  =  Obstical(\"obstical_{}\".format(self.obstical_count), self.x_max, self.x_min, self.y_max, self.y_min) \n",
    "            obstical_obj.set_position(obstical_x, obstical_y)\n",
    "            self.elements.append(obstical_obj)\n",
    "            self.draw_elements_on_canvas()\n",
    "        return self.canvas\n",
    "\n",
    "    def set_goal(self,points):\n",
    "        for flag_x, flag_y in  points:\n",
    "            flag_obj = Flag(\"flag_{}\".format(self.obstical_count), self.x_max, self.x_min, self.y_max, self.y_min) \n",
    "            flag_obj.set_position(flag_x, flag_y)\n",
    "            self.elements.append(flag_obj)\n",
    "            self.draw_elements_on_canvas()\n",
    "        return self.canvas\n",
    "    \n",
    "    def draw_elements_on_canvas(self):\n",
    "        # Init the canvas \n",
    "        self.canvas = np.ones(self.observation_shape) * 1\n",
    "\n",
    "        # Draw the heliopter on canvas\n",
    "        for elem in self.elements:\n",
    "            elem_shape = elem.icon.shape\n",
    "            x,y = elem.x, elem.y\n",
    "            self.canvas[y : y + elem_shape[1], x:x + elem_shape[0]] = elem.icon\n",
    "\n",
    "        text = 'Fuel Left: {} | Rewards: {}'.format(self.fuel_left, self.ep_return)\n",
    "\n",
    "        # Put the info on canvas \n",
    "        self.canvas = cv2.putText(self.canvas, text, (10,20), font,  \n",
    "                   0.8, (0,0,0), 1, cv2.LINE_AA)\n",
    "        \n",
    "    def get_action_meanings(self):\n",
    "        return {0: \"Right\", 1: \"Left\", 2: \"Down\", 3: \"Up\", 4: \"Do Nothing\"}\n",
    "    \n",
    "    def render(self, mode = \"human\"):\n",
    "        assert mode in [\"human\", \"rgb_array\"], \"Invalid mode, must be either \\\"human\\\" or \\\"rgb_array\\\"\"\n",
    "        if mode == \"human\":\n",
    "            cv2.imshow(\"Obstacle Avoidance Robot\", self.canvas)\n",
    "            cv2.waitKey(10)\n",
    "\n",
    "        elif mode == \"rgb_array\":\n",
    "            return self.canvas\n",
    "        \n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    def has_collided(self, elem1, elem2):\n",
    "        x_col = False\n",
    "        y_col = False\n",
    "\n",
    "        elem1_x, elem1_y = elem1.get_position()\n",
    "        elem2_x, elem2_y = elem2.get_position()\n",
    "\n",
    "        if 2 * abs(elem1_x - elem2_x) <= (elem1.icon_w + elem2.icon_w):\n",
    "            x_col =  True\n",
    "\n",
    "        if 2 * abs(elem1_y - elem2_y) <= (elem1.icon_h + elem2.icon_h):\n",
    "            y_col = True\n",
    "\n",
    "        if x_col and y_col:\n",
    "            return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Robot_Env()\n",
    "obs = env.reset()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_obstacle_list = [ (200,100), (400,150),(400, 110), \n",
    "                 (430, 390), (201, 304), (135, 281), \n",
    "                 (286, 373), (175, 280), (250, 375), (139, 327), (369, 278), \n",
    "                 (295, 196), (210, 111),(110,110)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstical_env = env.set_obsticals(full_obstacle_list)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(obstical_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goal_pos = [[700,700]]\n",
    "obstical_env = env.set_goal(goal_pos)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(obstical_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "env = Robot_Env()\n",
    "obs = env.reset()\n",
    "\n",
    "goal_pos = [[380,380]]\n",
    "full_obstacle_list = [ (200,100), (400,150),(400, 110), \n",
    "                 (430, 390), (201, 304), (135, 281), \n",
    "                 (286, 373), (175, 280), (250, 375), (139, 327), (369, 278), \n",
    "                 (295, 196), (210, 111),(110,110)]\n",
    "\n",
    "\n",
    "env.set_obsticals(full_obstacle_list)\n",
    "env.set_goal(goal_pos)\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Render the game\n",
    "    #env.render()\n",
    "    \n",
    "    if done == True:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(image_frame):\n",
    "    \n",
    "   \n",
    "    I = image_frame[20:] # Crop\n",
    "    \n",
    "    #print(\"Croped Image size : \",I.shape)\n",
    "    I = cv2.resize(I,(128,128))\n",
    "    return I.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "action = env.action_space.sample()\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"Action : \",action)\n",
    "print(\"Reward : \",reward)\n",
    "print(\"is done : \",done)\n",
    "print(\"info : \",info)\n",
    "print(\"Image size : \",obs.shape)\n",
    "plt.imshow(obs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = obs\n",
    "pre_process_img = pre_process(test_img)\n",
    "print(\"Pre process image : \",pre_process_img.shape)\n",
    "plt.imshow(pre_process_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 5\n",
    "MAX_ITERS= 2500\n",
    "learning_rate=1e-3\n",
    "IMG_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    pre_train = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet',input_shape= (IMG_SIZE, IMG_SIZE,3))\n",
    "    x= pre_train.output\n",
    "    x =  tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation=\"relu\",name='hidden_layer1')(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation=\"relu\",name='hidden_layer2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\",name='hidden_layer3')(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\",name='hidden_layer4')(x)\n",
    "    preds =tf.keras.layers.Dense(n_actions,activation='sigmoid', name='output')(x)\n",
    "    model=tf.keras.Model(inputs=pre_train.input,outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = x - np.mean(x)\n",
    "    x =  x / np.std(x)\n",
    "    \n",
    "    return x.astype(np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNan(val):\n",
    "     return val != val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    \n",
    "    R =  0 \n",
    "    for t in reversed(range (0,len(rewards))):\n",
    "        \n",
    "        if rewards[t] != 0 :\n",
    "            R = 0\n",
    "        R = R * gamma + rewards[t]\n",
    "        \n",
    "        \n",
    "        discounted_rewards[t] = R\n",
    "        \n",
    "    return normalize(discounted_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "        \n",
    "    \n",
    "    def clear(self):\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def add_to_memory(self,new_observation,new_action,new_reward):\n",
    "        self.observations.append(new_observation)\n",
    "        \n",
    "        self.actions.append(new_action)\n",
    "        \n",
    "        self.rewards.append(new_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(model,observation):\n",
    "    \n",
    "    observation = np.expand_dims(observation,axis = 0)\n",
    "    \n",
    "    logits = model.predict(observation)\n",
    "    \n",
    "    prob_weights = tf.nn.softmax(logits).numpy()\n",
    "    \n",
    "    isContainNan =  [ isNan(x) for x in prob_weights.flatten()]\n",
    "    \n",
    "    if True in   isContainNan:\n",
    "        return np.random.choice(n_actions,size = 1)[0]\n",
    "    \n",
    "    action = np.random.choice(n_actions,size = 1, p = prob_weights.flatten())[0]\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss function ###\n",
    "def compute_loss(logits,actions,rewards):\n",
    "    neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits,labels = actions)\n",
    "\n",
    "    loss = tf.reduce_mean(neg_logprob * rewards)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import time\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory:\n",
    "    def __init__(self, smoothing_factor=0.0):\n",
    "        self.alpha = smoothing_factor\n",
    "        self.loss = []\n",
    "    def append(self, value):\n",
    "        self.loss.append( self.alpha*self.loss[-1] + (1-self.alpha)*value if len(self.loss)>0 else value )\n",
    "    def get(self):\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicPlotter:\n",
    "    def __init__(self, sec, xlabel='', ylabel='', scale=None):\n",
    "\n",
    "        self.xlabel = xlabel\n",
    "        self.ylabel = ylabel\n",
    "        self.sec = sec\n",
    "        self.scale = scale\n",
    "\n",
    "        self.tic = time.time()\n",
    "\n",
    "    def plot(self, data):\n",
    "        if time.time() - self.tic > self.sec:\n",
    "            plt.cla()\n",
    "\n",
    "            if self.scale is None:\n",
    "                plt.plot(data)\n",
    "            elif self.scale == 'semilogx':\n",
    "                plt.semilogx(data)\n",
    "            elif self.scale == 'semilogy':\n",
    "                plt.semilogy(data)\n",
    "            elif self.scale == 'loglog':\n",
    "                plt.loglog(data)\n",
    "            else:\n",
    "                raise ValueError(\"unrecognized parameter scale {}\".format(self.scale))\n",
    "\n",
    "            plt.xlabel(self.xlabel); plt.ylabel(self.ylabel)\n",
    "            ipythondisplay.clear_output(wait=True)\n",
    "            ipythondisplay.display(plt.gcf())\n",
    " \n",
    "            self.tic = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "smoothed_reward = LossHistory(smoothing_factor=0.9)\n",
    "plotter = PeriodicPlotter(sec=5, xlabel='Iterations', ylabel='Rewards')\n",
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training step (forward and backpropagation) ###\n",
    "def train_step(model,optimizer,observations,actions,discounted_rewards):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(observations)\n",
    "        \n",
    "        loss = compute_loss(logits,actions,discounted_rewards)\n",
    "        #wandb.log({\"loss\": loss}) \n",
    "    grads = tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(MAX_ITERS):\n",
    "    plotter.plot(smoothed_reward.get())\n",
    "    \n",
    "    # Restart the environment\n",
    "    observation = env.reset()\n",
    "    env.set_obsticals(full_obstacle_list)\n",
    "    env.set_goal(goal_pos)\n",
    "\n",
    "    previous_frame = pre_process(observation)\n",
    "    \n",
    "    while True :\n",
    "        current_frame =  pre_process(observation)\n",
    "        \n",
    "        obs_change = current_frame - previous_frame\n",
    "        \n",
    "        action = choose_action(model,obs_change)\n",
    "        #env.render()\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.add_to_memory(obs_change,action,reward)\n",
    "        \n",
    "        if done :\n",
    "            \n",
    "            total_reward = sum(memory.rewards)\n",
    "            smoothed_reward.append(total_reward)\n",
    "            #wandb.log({\"total_reward\": total_reward})\n",
    "            train_step(model,\n",
    "                      optimizer,\n",
    "                      observations = np.stack(memory.observations,0),\n",
    "                      actions = np.array(memory.actions),\n",
    "                      discounted_rewards = discount_rewards(memory.rewards))\n",
    "            memory.clear()\n",
    "            break\n",
    "        \n",
    "        observation = next_observation\n",
    "        previous_frame = current_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"static-7_4_2022.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
